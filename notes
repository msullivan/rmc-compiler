The general intuition is that pushes become syncs; visibility edges
become lwsyncs; and execution edges become either control-isyncs, or
data or address dependencies.  There are some subtleties though.
Ignoring read-modify-writes, the algorithm is as follows:

1. Take the transitive closure of the visibility edges, and take the
    transitive closure of the execution edges.
2. Discard any edges involving a nop.
3. For all triples of the form i -vo-> push -xo/vo-> i', create a push
    edge i -push-> i'.  (These exist in the algorithm only, not in the
    theory.)
4. Discard any edges involving a push.
5. Insert synchronization into the code such that:

    a. Every push edge is cut by a sync.
    b. Every W->W and R->W visibility edge is cut by a sync or lwsync.
    c. Every R->R edge is cut by a sync, lwsync,
       control-isync, or data/address dependency.
    d. Every R->W and execution edge is cut by a sync, lwsync,
       control-isync, data/address dependency, or control dependency.

    Doing so is interesting, particularly when edges cross from one
    basic block to another.

    Also, the execution stuff is a good deal trickier than that
    because of edges across loops. (And there are /always/ edges
    across loops, because the function will get called again!)

    Note that:

    a. R->R visibility edges have no more force than execution (other
       than transitive implications).
    b. W->R visibility and execution edges, and W->W execution edges,
       are discarded (after transitive closure).
    c. Pushes without both a visibility predecessor and a
       visibility/execution successor are discarded.


--
pre and post edges we will just add the stuff and then try to notice it.
 * False; we don't actually do it that way.

honestly maybe that's what we should do for push also? although, won't
actually be hard to do push right, but maybe not worth the state space
blowup.
 * But we do do it that way for push.

Should be a little careful because probably a common idiom will be:
  L(push, PUSH); VEDGE(pre, push); XEDGE(push, post);
and we should just generate a single sync in place and not do dumb things.

It will be common because I'll probably write a macro for it, so...

Will actually need special handling of pre/push to posts even just for
correctness!


--
I sort of dislike inserting all these barriers so early. I feel like
it might inhibit llvm more than we want.
--

But since we *do* insert all the barriers early, we actually can
implement pre and post by making dummy actions immediately before and
after actions with pre/post constraints. The only tricky thing is that
we need to make sure we don't try any funny business with dependencies
for xo edges.


------------------------------------------------------
Things about the paper:
  -- we need pushes for two threads writing to locations to get ordering:
     maybe we should require co+vo to be acyclic? is this important
  -- ISA2+lwsync+data+addr: our semantics don't allow us to generate this
      code and be correct; vo+xo+xo would allow the forbidden behavior;
      vo+vo+xo works, as does push+xo+xo;
      push+xo+xo is maaaybe what we want on ARM, but vo+vo+xo is
      definitely better on x86


--------------------------------------------

I think we are going to want to be able to adjust where labels get
"bound": that is, relative to loops, the function call, etc. This
seems important for doing consume style stuff. The thing that matches
the theory would be to handle it at the label declaration, but it
seems cleaner for code implementation to do it when specifying edges.

We *definitely* want to be able to bind inside a function. Loops more
complicated. What about gotos, etc.

Actually maybe not definitely. If rcu_read_unlock() has a barrier then
maybe we don't give a shit anymore.

*** Idea: allow explicitly specifying a binding location for a
label. That binding location then needs to dominate all occurances of
the label as well as anything with edges to/from that label. Then,
when looking at paths, we can ignore paths that go through the binding
site, I think. Not totally sure how to properly handle differing
binding sites between src/dst (technically they will probably always
differ, even if trivially...): maybe ignore the least common ancestor
dominator.

Probably also want a way to specify binding something at the "most
narrow possible binding" (least common dominator?).



--- How elaborate of real dependencies do we want to try to preserve?
And how do we preserve them?  Detecting and preserving straightforward
deps should be eaaaaasy.

We probably need to be able to handle linux RCU list style things,
which means handling bitcasts and GEPs.

DONE *** We are definitely definitely going to need a notion of path dependence
for detecting dependency. A super common example of RCU is searching a
linked list, so...
- Wee.

DONE * I should run some benchmarks to see how much faster it is with RCU.
- Like twice as fast


-----

** Combining isync and depedencies might actually be quite profitable,
since all addresses need to be resolved, right?


----

DONE *** We need to adjust our notion of cost to take loops into account!
We will basically preferentially put things into loops instead of
taking them out, which is totally backwards.  Dunno if it should be
part of capacity or a seperate score.  Maybe increase the cost by a
factor of 4^n where n is the loop nesting count?  (4 because a loop
will probably have 1/2 the capacity of its parent; 4^n makes a loop
have twice the cost of its parent).


---

*** We could track which functions always have barriers (dmbs, isyncs)
    and factor that information in when doing analysis.


---

Hilariously, calling mprotect

--

In C++: 1.10.28: "An implementation should ensure that the last value (in modification order) assigned by an atomic or
synchronization operation will become visible to all other threads in a finite period of time."
This precludes (I think) transformations like (assuming mo_relaxed):
x = 1; while (y); --> while (y); x = 1;

We don't really have a story for this sort of thing. I was originally
thinking that I wanted to use W->R execution order edges to prevent
this, but I am less sure, now. Maybe it should just be a side condition
in general that things "eventually" execute, "eventually" become visible.

I don't think that this side condition is that much painful than the
other, saves us annotations, and I *think* already fits our compiler
strategy (since we are turning all our accesses into mo_relaxed).

(RMW related things are complicating this analysis.)

--

Do we have a theory story for using weak cmpxchg?
Maybe:
weak_cmpxchg(p, ev, nv) = RMW(p, x. if x == ev && !spurious() then nv else x)
Does this work? Is ordering stuff reasonable?
Oh, it also doesn't quite work, since it needs to output whether it succeeded, which can't be
divined from the value.

XXX: Actually, does /strong/ cmpxchg work right in failure cases??
The RW semantics in RMC /always/ does a write, but that won't actually
be the case, right? Does this cause trouble?
cmpxchg(p, ev, nv) = RMW(p, x. if x == ev then nv else x)

Implementing CAS with RMW can result in rf edges that don't actually
correspond with real rf edges in the hardware. I'm not totally sure
what all implications this has. The most obvious one is that it can
establish visibility in places where it might not make sense. I
/think/ that is fixed on ARM by making sure that all xo edges /out/ of
a CAS are dmbs (and not something dep based). On POWER it is a little worse:
all vo edges into and xo edges out of need to be *sync* (not lwsync).

This can be fixed by adding a new CAS directly to the language; like
RMW, it will evaluate into RWs and speculations.

---


DONE *** Add some factor so that one lwsync is better than two, even if the
    cost is the same. Reduces code size, maybe inhibits optimizations
    less in certain cases?
*** Maybe we should be more directly merging
things when all the edges out of a node are cut.


---

We /do/ need to do value obfuscation or some such on data-dep
things. I suspected this but was hoping it wasn't true. It is going to
be annoying and I worry it will hurt optimizations noticably.

I /think/ that we can maybe get away with one obfuscation per chain,
at the source value?
That's not true: we could get screwed on a comparison from a derived
value.
 * Maybe obfuscate every time a value in the chain has multiple uses.
 * Maybe do some checking of how the values are used? Seems
   tricky. May need to follow far.

These obfuscating copies /will/ actually cause extra register copies,
etc, I think. Yeah, they definitely do!
*** Could we run another pass that removes all of our obfuscating
    copies? Or do they keep the backend stuff honest as well?
    It does seem necessary, at least to keep the POWER backend on -O3
	honest.

 * hideAllUses - well, only need to hide N-1, really

---

*** Need to think more about the atomic/non-atomic distinction and
optimizations. *Not* everything that is labeled needs to be atomic
(e.g reads from an RCU-protected pointer, the data in a standard MP
example) and some things that need to be atomic don't need to be
labeled (esp. when using pre/post). Maybe the thing to do is to
imitate C11 more and have rmc_store/rmc_load, maybe even _Rmc/Rmc<T>?


------

We should think about ARMv8s acquire/release operations.


-----

Running optimizations early can cause us some trouble, since inlining
might cause collision between label names, which could cause bad
codegen. Maybe we need a uniqifying pass that runs early or
something... but that's not great.

Actually, wait. What is the relationship between inlining and cross
function call edges. I think this can cause us problems:
void f() { L(a, FOO); VEDGE(a, a); }
void g() { f(); f_caller(); f(); }
could compile to
void f() { FOO; dmb; }
void g() { FOO; f_caller(); dmb; FOO; dmb; }
which is wrong.

... maybe make __attribute__((noinline)) the way we indicate that we
want outside binding...
RMC_BIND_OUTSIDE
* DONE


-----
Wait again. What is the relationship between /recursion/ and cross
call edges. Consider:
int f(int x, int y) {
	VEDGE(a, b);
	if (x) { L(a, ...); }
	if (something) { f(0, 1); }
	if (y) { L(b, ...); }
}
Honestly, most reasonable compilation things seem like they'll just
work fine, but...

I guess we could add edges from all function calls that could possibly
invoke us recursively... but that sounds annoying.

* I think for now we will not support this???

-----

*** Treat CAS operations as having a built in control dependency??
*** Could cut down on how many dummy copies we have when doing control
    dep stuff. Could LLVM merge two relaxed loads? That could cause us
    trouble. (But mark it volatile?)

*** But actually maybe we need *more* copies. Maybe we need to
    hideAllUses of the load to avoid other data getting pulled in...
